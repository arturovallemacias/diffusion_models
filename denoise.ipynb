{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJK0QGhXMnXs1CyldeGkCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturovallemacias/diffusion_models/blob/main/denoise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USERNAME=\"arturovallemacias\"\n",
        "TOKEN=\"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\"\n",
        "\n",
        "# Configura el helper de credenciales para almacenarlas en cachÃ©\n",
        "!git config --global credential.helper store\n",
        "\n",
        "# Clona el repositorio utilizando el token personal\n",
        "!git clone https://$USERNAME:$TOKEN@github.com/$USERNAME/diffusion_models.git\n",
        "\n",
        "\n",
        "!git config --global user.email \"arturo_valle@live.com\"\n",
        "!git config --global user.name \"arturovallemacias\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERGNQhDlFSnM",
        "outputId": "f1286b9c-452c-4ff4-f1fd-0f04890da29e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion_models'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 98 (delta 28), reused 25 (delta 25), pack-reused 66\u001b[K\n",
            "Receiving objects: 100% (98/98), 4.31 MiB | 16.00 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/diffusion_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UssCnbAYF8fr",
        "outputId": "f6541b1a-484d-4db9-8031-cf294cbc1d2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusion_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "from unet import UNet\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M1qQUZaKFBo0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=6, sci_mode=False)\n",
        "np.set_printoptions(precision=9, suppress=True)"
      ],
      "metadata": {
        "id": "VsENjUHpA6n3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "trainX = np.float32(trainX) / 255.\n",
        "testX = np.float32(testX) / 255."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLT4XmgJGBKr",
        "outputId": "6d9ecab6-8b10-4bd4-f226-144d4c345aa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_batch(batch_size, device):\n",
        "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
        "    data = torch.from_numpy(trainX[indices]).unsqueeze(1).to(device)\n",
        "    return torch.nn.functional.interpolate(data, 32)"
      ],
      "metadata": {
        "id": "oT16dz_NGXlL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel():\n",
        "\n",
        "    def __init__(self, T : int, model : nn.Module, device : str):\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        self.function_approximator = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        self.beta = torch.linspace(1e-4, 0.02, T).to(device)\n",
        "        print(self.beta)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def training(self, batch_size, optimizer):\n",
        "        \"\"\"\n",
        "        Algorithm 1 in Denoising Diffusion Probabilistic Models\n",
        "        \"\"\"\n",
        "\n",
        "        x0 = sample_batch(batch_size, self.device)\n",
        "\n",
        "        t = torch.randint(1, self.T + 1, (batch_size,), device=self.device, dtype=torch.long)\n",
        "\n",
        "        eps = torch.randn_like(x0)\n",
        "\n",
        "        # Take one gradient descent step\n",
        "        alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "        eps_predicted = self.function_approximator(torch.sqrt(\n",
        "            alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * eps, t-1)\n",
        "        loss = nn.functional.mse_loss(eps, eps_predicted)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), use_tqdm=True):\n",
        "\n",
        "        x = torch.randn((n_samples, image_channels, img_size[0], img_size[1]),\n",
        "                         device=self.device)\n",
        "\n",
        "        progress_bar = tqdm if use_tqdm else lambda x : x\n",
        "        for t in progress_bar(range(self.T, 0, -1)):\n",
        "            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
        "\n",
        "            t = torch.ones(n_samples, dtype=torch.long, device=self.device) * t\n",
        "\n",
        "            beta_t = self.beta[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_t = self.alpha[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "            mean = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(\n",
        "                1 - alpha_bar_t)) * self.function_approximator(x, t-1))\n",
        "            sigma = torch.sqrt(beta_t)\n",
        "            x = mean + sigma * z\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3y3iXpM28w9W"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "model = UNet()\n",
        "device = \"cpu\"\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "diffusion_model = DiffusionModel(1000, model, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "-y0YJJMM9G13",
        "outputId": "6072018d-e337-43eb-ca57-0a977a581553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0.000100,     0.000120,     0.000140,     0.000160,     0.000180,\n",
            "            0.000200,     0.000220,     0.000239,     0.000259,     0.000279,\n",
            "            0.000299,     0.000319,     0.000339,     0.000359,     0.000379,\n",
            "            0.000399,     0.000419,     0.000439,     0.000459,     0.000478,\n",
            "            0.000498,     0.000518,     0.000538,     0.000558,     0.000578,\n",
            "            0.000598,     0.000618,     0.000638,     0.000658,     0.000678,\n",
            "            0.000698,     0.000718,     0.000737,     0.000757,     0.000777,\n",
            "            0.000797,     0.000817,     0.000837,     0.000857,     0.000877,\n",
            "            0.000897,     0.000917,     0.000937,     0.000957,     0.000976,\n",
            "            0.000996,     0.001016,     0.001036,     0.001056,     0.001076,\n",
            "            0.001096,     0.001116,     0.001136,     0.001156,     0.001176,\n",
            "            0.001196,     0.001216,     0.001235,     0.001255,     0.001275,\n",
            "            0.001295,     0.001315,     0.001335,     0.001355,     0.001375,\n",
            "            0.001395,     0.001415,     0.001435,     0.001455,     0.001474,\n",
            "            0.001494,     0.001514,     0.001534,     0.001554,     0.001574,\n",
            "            0.001594,     0.001614,     0.001634,     0.001654,     0.001674,\n",
            "            0.001694,     0.001714,     0.001733,     0.001753,     0.001773,\n",
            "            0.001793,     0.001813,     0.001833,     0.001853,     0.001873,\n",
            "            0.001893,     0.001913,     0.001933,     0.001953,     0.001972,\n",
            "            0.001992,     0.002012,     0.002032,     0.002052,     0.002072,\n",
            "            0.002092,     0.002112,     0.002132,     0.002152,     0.002172,\n",
            "            0.002192,     0.002212,     0.002231,     0.002251,     0.002271,\n",
            "            0.002291,     0.002311,     0.002331,     0.002351,     0.002371,\n",
            "            0.002391,     0.002411,     0.002431,     0.002451,     0.002470,\n",
            "            0.002490,     0.002510,     0.002530,     0.002550,     0.002570,\n",
            "            0.002590,     0.002610,     0.002630,     0.002650,     0.002670,\n",
            "            0.002690,     0.002710,     0.002729,     0.002749,     0.002769,\n",
            "            0.002789,     0.002809,     0.002829,     0.002849,     0.002869,\n",
            "            0.002889,     0.002909,     0.002929,     0.002949,     0.002968,\n",
            "            0.002988,     0.003008,     0.003028,     0.003048,     0.003068,\n",
            "            0.003088,     0.003108,     0.003128,     0.003148,     0.003168,\n",
            "            0.003188,     0.003208,     0.003227,     0.003247,     0.003267,\n",
            "            0.003287,     0.003307,     0.003327,     0.003347,     0.003367,\n",
            "            0.003387,     0.003407,     0.003427,     0.003447,     0.003466,\n",
            "            0.003486,     0.003506,     0.003526,     0.003546,     0.003566,\n",
            "            0.003586,     0.003606,     0.003626,     0.003646,     0.003666,\n",
            "            0.003686,     0.003706,     0.003725,     0.003745,     0.003765,\n",
            "            0.003785,     0.003805,     0.003825,     0.003845,     0.003865,\n",
            "            0.003885,     0.003905,     0.003925,     0.003945,     0.003964,\n",
            "            0.003984,     0.004004,     0.004024,     0.004044,     0.004064,\n",
            "            0.004084,     0.004104,     0.004124,     0.004144,     0.004164,\n",
            "            0.004184,     0.004204,     0.004223,     0.004243,     0.004263,\n",
            "            0.004283,     0.004303,     0.004323,     0.004343,     0.004363,\n",
            "            0.004383,     0.004403,     0.004423,     0.004443,     0.004462,\n",
            "            0.004482,     0.004502,     0.004522,     0.004542,     0.004562,\n",
            "            0.004582,     0.004602,     0.004622,     0.004642,     0.004662,\n",
            "            0.004682,     0.004702,     0.004721,     0.004741,     0.004761,\n",
            "            0.004781,     0.004801,     0.004821,     0.004841,     0.004861,\n",
            "            0.004881,     0.004901,     0.004921,     0.004941,     0.004960,\n",
            "            0.004980,     0.005000,     0.005020,     0.005040,     0.005060,\n",
            "            0.005080,     0.005100,     0.005120,     0.005140,     0.005160,\n",
            "            0.005180,     0.005199,     0.005219,     0.005239,     0.005259,\n",
            "            0.005279,     0.005299,     0.005319,     0.005339,     0.005359,\n",
            "            0.005379,     0.005399,     0.005419,     0.005439,     0.005458,\n",
            "            0.005478,     0.005498,     0.005518,     0.005538,     0.005558,\n",
            "            0.005578,     0.005598,     0.005618,     0.005638,     0.005658,\n",
            "            0.005678,     0.005697,     0.005717,     0.005737,     0.005757,\n",
            "            0.005777,     0.005797,     0.005817,     0.005837,     0.005857,\n",
            "            0.005877,     0.005897,     0.005917,     0.005937,     0.005956,\n",
            "            0.005976,     0.005996,     0.006016,     0.006036,     0.006056,\n",
            "            0.006076,     0.006096,     0.006116,     0.006136,     0.006156,\n",
            "            0.006176,     0.006195,     0.006215,     0.006235,     0.006255,\n",
            "            0.006275,     0.006295,     0.006315,     0.006335,     0.006355,\n",
            "            0.006375,     0.006395,     0.006415,     0.006435,     0.006454,\n",
            "            0.006474,     0.006494,     0.006514,     0.006534,     0.006554,\n",
            "            0.006574,     0.006594,     0.006614,     0.006634,     0.006654,\n",
            "            0.006674,     0.006693,     0.006713,     0.006733,     0.006753,\n",
            "            0.006773,     0.006793,     0.006813,     0.006833,     0.006853,\n",
            "            0.006873,     0.006893,     0.006913,     0.006933,     0.006952,\n",
            "            0.006972,     0.006992,     0.007012,     0.007032,     0.007052,\n",
            "            0.007072,     0.007092,     0.007112,     0.007132,     0.007152,\n",
            "            0.007172,     0.007191,     0.007211,     0.007231,     0.007251,\n",
            "            0.007271,     0.007291,     0.007311,     0.007331,     0.007351,\n",
            "            0.007371,     0.007391,     0.007411,     0.007431,     0.007450,\n",
            "            0.007470,     0.007490,     0.007510,     0.007530,     0.007550,\n",
            "            0.007570,     0.007590,     0.007610,     0.007630,     0.007650,\n",
            "            0.007670,     0.007689,     0.007709,     0.007729,     0.007749,\n",
            "            0.007769,     0.007789,     0.007809,     0.007829,     0.007849,\n",
            "            0.007869,     0.007889,     0.007909,     0.007929,     0.007948,\n",
            "            0.007968,     0.007988,     0.008008,     0.008028,     0.008048,\n",
            "            0.008068,     0.008088,     0.008108,     0.008128,     0.008148,\n",
            "            0.008168,     0.008187,     0.008207,     0.008227,     0.008247,\n",
            "            0.008267,     0.008287,     0.008307,     0.008327,     0.008347,\n",
            "            0.008367,     0.008387,     0.008407,     0.008427,     0.008446,\n",
            "            0.008466,     0.008486,     0.008506,     0.008526,     0.008546,\n",
            "            0.008566,     0.008586,     0.008606,     0.008626,     0.008646,\n",
            "            0.008666,     0.008685,     0.008705,     0.008725,     0.008745,\n",
            "            0.008765,     0.008785,     0.008805,     0.008825,     0.008845,\n",
            "            0.008865,     0.008885,     0.008905,     0.008925,     0.008944,\n",
            "            0.008964,     0.008984,     0.009004,     0.009024,     0.009044,\n",
            "            0.009064,     0.009084,     0.009104,     0.009124,     0.009144,\n",
            "            0.009164,     0.009183,     0.009203,     0.009223,     0.009243,\n",
            "            0.009263,     0.009283,     0.009303,     0.009323,     0.009343,\n",
            "            0.009363,     0.009383,     0.009403,     0.009423,     0.009442,\n",
            "            0.009462,     0.009482,     0.009502,     0.009522,     0.009542,\n",
            "            0.009562,     0.009582,     0.009602,     0.009622,     0.009642,\n",
            "            0.009662,     0.009681,     0.009701,     0.009721,     0.009741,\n",
            "            0.009761,     0.009781,     0.009801,     0.009821,     0.009841,\n",
            "            0.009861,     0.009881,     0.009901,     0.009921,     0.009940,\n",
            "            0.009960,     0.009980,     0.010000,     0.010020,     0.010040,\n",
            "            0.010060,     0.010080,     0.010100,     0.010120,     0.010140,\n",
            "            0.010160,     0.010179,     0.010199,     0.010219,     0.010239,\n",
            "            0.010259,     0.010279,     0.010299,     0.010319,     0.010339,\n",
            "            0.010359,     0.010379,     0.010399,     0.010419,     0.010438,\n",
            "            0.010458,     0.010478,     0.010498,     0.010518,     0.010538,\n",
            "            0.010558,     0.010578,     0.010598,     0.010618,     0.010638,\n",
            "            0.010658,     0.010677,     0.010697,     0.010717,     0.010737,\n",
            "            0.010757,     0.010777,     0.010797,     0.010817,     0.010837,\n",
            "            0.010857,     0.010877,     0.010897,     0.010917,     0.010936,\n",
            "            0.010956,     0.010976,     0.010996,     0.011016,     0.011036,\n",
            "            0.011056,     0.011076,     0.011096,     0.011116,     0.011136,\n",
            "            0.011156,     0.011175,     0.011195,     0.011215,     0.011235,\n",
            "            0.011255,     0.011275,     0.011295,     0.011315,     0.011335,\n",
            "            0.011355,     0.011375,     0.011395,     0.011415,     0.011434,\n",
            "            0.011454,     0.011474,     0.011494,     0.011514,     0.011534,\n",
            "            0.011554,     0.011574,     0.011594,     0.011614,     0.011634,\n",
            "            0.011654,     0.011673,     0.011693,     0.011713,     0.011733,\n",
            "            0.011753,     0.011773,     0.011793,     0.011813,     0.011833,\n",
            "            0.011853,     0.011873,     0.011893,     0.011913,     0.011932,\n",
            "            0.011952,     0.011972,     0.011992,     0.012012,     0.012032,\n",
            "            0.012052,     0.012072,     0.012092,     0.012112,     0.012132,\n",
            "            0.012152,     0.012171,     0.012191,     0.012211,     0.012231,\n",
            "            0.012251,     0.012271,     0.012291,     0.012311,     0.012331,\n",
            "            0.012351,     0.012371,     0.012391,     0.012411,     0.012430,\n",
            "            0.012450,     0.012470,     0.012490,     0.012510,     0.012530,\n",
            "            0.012550,     0.012570,     0.012590,     0.012610,     0.012630,\n",
            "            0.012650,     0.012669,     0.012689,     0.012709,     0.012729,\n",
            "            0.012749,     0.012769,     0.012789,     0.012809,     0.012829,\n",
            "            0.012849,     0.012869,     0.012889,     0.012909,     0.012928,\n",
            "            0.012948,     0.012968,     0.012988,     0.013008,     0.013028,\n",
            "            0.013048,     0.013068,     0.013088,     0.013108,     0.013128,\n",
            "            0.013148,     0.013167,     0.013187,     0.013207,     0.013227,\n",
            "            0.013247,     0.013267,     0.013287,     0.013307,     0.013327,\n",
            "            0.013347,     0.013367,     0.013387,     0.013407,     0.013426,\n",
            "            0.013446,     0.013466,     0.013486,     0.013506,     0.013526,\n",
            "            0.013546,     0.013566,     0.013586,     0.013606,     0.013626,\n",
            "            0.013646,     0.013665,     0.013685,     0.013705,     0.013725,\n",
            "            0.013745,     0.013765,     0.013785,     0.013805,     0.013825,\n",
            "            0.013845,     0.013865,     0.013885,     0.013905,     0.013924,\n",
            "            0.013944,     0.013964,     0.013984,     0.014004,     0.014024,\n",
            "            0.014044,     0.014064,     0.014084,     0.014104,     0.014124,\n",
            "            0.014144,     0.014163,     0.014183,     0.014203,     0.014223,\n",
            "            0.014243,     0.014263,     0.014283,     0.014303,     0.014323,\n",
            "            0.014343,     0.014363,     0.014383,     0.014403,     0.014422,\n",
            "            0.014442,     0.014462,     0.014482,     0.014502,     0.014522,\n",
            "            0.014542,     0.014562,     0.014582,     0.014602,     0.014622,\n",
            "            0.014642,     0.014661,     0.014681,     0.014701,     0.014721,\n",
            "            0.014741,     0.014761,     0.014781,     0.014801,     0.014821,\n",
            "            0.014841,     0.014861,     0.014881,     0.014900,     0.014920,\n",
            "            0.014940,     0.014960,     0.014980,     0.015000,     0.015020,\n",
            "            0.015040,     0.015060,     0.015080,     0.015100,     0.015120,\n",
            "            0.015140,     0.015159,     0.015179,     0.015199,     0.015219,\n",
            "            0.015239,     0.015259,     0.015279,     0.015299,     0.015319,\n",
            "            0.015339,     0.015359,     0.015379,     0.015398,     0.015418,\n",
            "            0.015438,     0.015458,     0.015478,     0.015498,     0.015518,\n",
            "            0.015538,     0.015558,     0.015578,     0.015598,     0.015618,\n",
            "            0.015638,     0.015657,     0.015677,     0.015697,     0.015717,\n",
            "            0.015737,     0.015757,     0.015777,     0.015797,     0.015817,\n",
            "            0.015837,     0.015857,     0.015877,     0.015896,     0.015916,\n",
            "            0.015936,     0.015956,     0.015976,     0.015996,     0.016016,\n",
            "            0.016036,     0.016056,     0.016076,     0.016096,     0.016116,\n",
            "            0.016136,     0.016155,     0.016175,     0.016195,     0.016215,\n",
            "            0.016235,     0.016255,     0.016275,     0.016295,     0.016315,\n",
            "            0.016335,     0.016355,     0.016375,     0.016394,     0.016414,\n",
            "            0.016434,     0.016454,     0.016474,     0.016494,     0.016514,\n",
            "            0.016534,     0.016554,     0.016574,     0.016594,     0.016614,\n",
            "            0.016634,     0.016653,     0.016673,     0.016693,     0.016713,\n",
            "            0.016733,     0.016753,     0.016773,     0.016793,     0.016813,\n",
            "            0.016833,     0.016853,     0.016873,     0.016892,     0.016912,\n",
            "            0.016932,     0.016952,     0.016972,     0.016992,     0.017012,\n",
            "            0.017032,     0.017052,     0.017072,     0.017092,     0.017112,\n",
            "            0.017132,     0.017151,     0.017171,     0.017191,     0.017211,\n",
            "            0.017231,     0.017251,     0.017271,     0.017291,     0.017311,\n",
            "            0.017331,     0.017351,     0.017371,     0.017390,     0.017410,\n",
            "            0.017430,     0.017450,     0.017470,     0.017490,     0.017510,\n",
            "            0.017530,     0.017550,     0.017570,     0.017590,     0.017610,\n",
            "            0.017630,     0.017649,     0.017669,     0.017689,     0.017709,\n",
            "            0.017729,     0.017749,     0.017769,     0.017789,     0.017809,\n",
            "            0.017829,     0.017849,     0.017869,     0.017888,     0.017908,\n",
            "            0.017928,     0.017948,     0.017968,     0.017988,     0.018008,\n",
            "            0.018028,     0.018048,     0.018068,     0.018088,     0.018108,\n",
            "            0.018128,     0.018147,     0.018167,     0.018187,     0.018207,\n",
            "            0.018227,     0.018247,     0.018267,     0.018287,     0.018307,\n",
            "            0.018327,     0.018347,     0.018367,     0.018386,     0.018406,\n",
            "            0.018426,     0.018446,     0.018466,     0.018486,     0.018506,\n",
            "            0.018526,     0.018546,     0.018566,     0.018586,     0.018606,\n",
            "            0.018626,     0.018645,     0.018665,     0.018685,     0.018705,\n",
            "            0.018725,     0.018745,     0.018765,     0.018785,     0.018805,\n",
            "            0.018825,     0.018845,     0.018865,     0.018884,     0.018904,\n",
            "            0.018924,     0.018944,     0.018964,     0.018984,     0.019004,\n",
            "            0.019024,     0.019044,     0.019064,     0.019084,     0.019104,\n",
            "            0.019124,     0.019143,     0.019163,     0.019183,     0.019203,\n",
            "            0.019223,     0.019243,     0.019263,     0.019283,     0.019303,\n",
            "            0.019323,     0.019343,     0.019363,     0.019382,     0.019402,\n",
            "            0.019422,     0.019442,     0.019462,     0.019482,     0.019502,\n",
            "            0.019522,     0.019542,     0.019562,     0.019582,     0.019602,\n",
            "            0.019622,     0.019641,     0.019661,     0.019681,     0.019701,\n",
            "            0.019721,     0.019741,     0.019761,     0.019781,     0.019801,\n",
            "            0.019821,     0.019841,     0.019861,     0.019880,     0.019900,\n",
            "            0.019920,     0.019940,     0.019960,     0.019980,     0.020000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Configurar la precisiÃ³n de impresiÃ³n para tensores\n",
        "torch.set_printoptions(precision=6, sci_mode=False)\n",
        "\n",
        "# Ejemplo de un tensor con valores pequeÃ±os\n",
        "tensor = torch.tensor([1.9940e-02, 1.9960e-02, 1.9980e-02, 2.0000e-02])\n",
        "\n",
        "# Imprimir el tensor con la nueva configuraciÃ³n\n",
        "print(tensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "355vAczVAdCG",
        "outputId": "0b9db90c-e23e-49ae-abb2-d50416ef9548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.019940, 0.019960, 0.019980, 0.020000])\n"
          ]
        }
      ]
    }
  ]
}