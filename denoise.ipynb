{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOIqbdiLDeJlx90XUK9Xfob",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturovallemacias/diffusion_models/blob/main/denoise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USERNAME=\"arturovallemacias\"\n",
        "TOKEN=\"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\"\n",
        "\n",
        "# Configura el helper de credenciales para almacenarlas en caché\n",
        "!git config --global credential.helper store\n",
        "\n",
        "# Clona el repositorio utilizando el token personal\n",
        "!git clone https://$USERNAME:$TOKEN@github.com/$USERNAME/diffusion_models.git\n",
        "\n",
        "\n",
        "!git config --global user.email \"arturo_valle@live.com\"\n",
        "!git config --global user.name \"arturovallemacias\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERGNQhDlFSnM",
        "outputId": "da93b316-6f93-4e77-f618-a5aacc43da25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion_models'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 110 (delta 35), reused 25 (delta 25), pack-reused 66\u001b[K\n",
            "Receiving objects: 100% (110/110), 4.34 MiB | 28.86 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/diffusion_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UssCnbAYF8fr",
        "outputId": "cd40de20-5b58-4711-a41c-868542bf3d48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusion_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M1qQUZaKFBo0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=6, sci_mode=False)\n",
        "np.set_printoptions(precision=9, suppress=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "VsENjUHpA6n3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "trainX = np.float32(trainX) / 255.\n",
        "testX = np.float32(testX) / 255."
      ],
      "metadata": {
        "id": "MLT4XmgJGBKr",
        "outputId": "2f355114-3683-4c4c-8e78-943ae3fcae33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_batch(batch_size, device):\n",
        "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
        "    data = torch.from_numpy(trainX[indices]).unsqueeze(1).to(device)\n",
        "    return torch.nn.functional.interpolate(data, 32)"
      ],
      "metadata": {
        "id": "oT16dz_NGXlL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
        "    \"\"\"\n",
        "    Retrieved from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py#LL90C1-L109C13\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
        "    emb = timesteps.type(torch.float32)[:, None] * emb[None, :]\n",
        "    emb = torch.concat([torch.sin(emb), torch.cos(emb)], axis=1)\n",
        "\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.pad(emb, [[0, 0], [0, 1]])\n",
        "\n",
        "    assert emb.shape == (timesteps.shape[0], embedding_dim), f\"{emb.shape}\"\n",
        "    return emb\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "\n",
        "    def __init__(self, C):\n",
        "        \"\"\"\n",
        "        :param C (int): number of input and output channels\n",
        "        \"\"\"\n",
        "        super(Downsample, self).__init__()\n",
        "        self.conv = nn.Conv2d(C, C, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.conv(x)\n",
        "        assert x.shape == (B, C, H // 2, W // 2)\n",
        "        return x\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "\n",
        "    def __init__(self, C):\n",
        "        \"\"\"\n",
        "        :param C (int): number of input and output channels\n",
        "        \"\"\"\n",
        "        super(Upsample, self).__init__()\n",
        "        self.conv = nn.Conv2d(C, C, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = nn.functional.interpolate(x, size=None, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv(x)\n",
        "        assert x.shape == (B, C, H * 2, W * 2)\n",
        "        return x\n",
        "\n",
        "class Nin(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, scale = 1e-10):\n",
        "        super(Nin, self).__init__()\n",
        "\n",
        "        n = (in_dim + out_dim) / 2\n",
        "        limit = np.sqrt(3 * scale / n)\n",
        "        self.W = torch.nn.Parameter(torch.zeros((in_dim, out_dim), dtype=torch.float32\n",
        "                                               ).uniform_(-limit, limit))\n",
        "        self.b = torch.nn.Parameter(torch.zeros((1, out_dim, 1, 1), dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum('bchw, co->bowh', x, self.W) + self.b\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch, dropout_rate=0.1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
        "        self.dense = nn.Linear(512, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1)\n",
        "\n",
        "        if not (in_ch == out_ch):\n",
        "            self.nin = Nin(in_ch, out_ch)\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.nonlinearity = torch.nn.SiLU()\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        :param x: (B, C, H, W)\n",
        "        :param temb: (B, dim)\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.nonlinearity(nn.functional.group_norm(x, num_groups=32))\n",
        "        h = self.conv1(x)\n",
        "\n",
        "        # add in timestep embedding\n",
        "        h += self.dense(self.nonlinearity(temb))[:, :, None, None]\n",
        "\n",
        "        h = self.nonlinearity(nn.functional.group_norm(h, num_groups=32))\n",
        "        h = nn.functional.dropout(h, p=self.dropout_rate)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if not (x.shape[1] == h.shape[1]):\n",
        "            x = self.nin(x)\n",
        "\n",
        "        assert x.shape == h.shape\n",
        "        return x + h\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, ch):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        self.Q = Nin(ch, ch)\n",
        "        self.K = Nin(ch, ch)\n",
        "        self.V = Nin(ch, ch)\n",
        "\n",
        "        self.ch = ch\n",
        "\n",
        "        self.nin = Nin(ch, ch, scale=0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        assert C == self.ch\n",
        "\n",
        "        h = nn.functional.group_norm(x, num_groups=32)\n",
        "        q = self.Q(h)\n",
        "        k = self.K(h)\n",
        "        v = self.V(h)\n",
        "\n",
        "        w = torch.einsum('bchw,bcHW->bhwHW', q, k) * (int(C) ** (-0.5)) # [B, H, W, H, W]\n",
        "        w = torch.reshape(w, [B, H, W, H * W])\n",
        "        w = torch.nn.functional.softmax(w, dim=-1)\n",
        "        w = torch.reshape(w, [B, H, W, H, W])\n",
        "\n",
        "        h = torch.einsum('bhwHW,bcHW->bchw', w, v)\n",
        "        h = self.nin(h)\n",
        "\n",
        "        assert h.shape == x.shape\n",
        "        return x + h\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, ch=128, in_ch=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.ch = ch\n",
        "        self.linear1 = nn.Linear(ch, 4 * ch)\n",
        "        self.linear2 = nn.Linear(4 * ch, 4 * ch)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_ch, ch, 3, stride=1, padding=1)\n",
        "\n",
        "        self.down = nn.ModuleList([ResNetBlock(ch, 1 * ch),\n",
        "                                   ResNetBlock(1 * ch, 1 * ch),\n",
        "                                   Downsample(1 * ch),\n",
        "                                   ResNetBlock(1 * ch, 2 * ch),\n",
        "                                   AttentionBlock(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   AttentionBlock(2 * ch),\n",
        "                                   Downsample(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   Downsample(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch)])\n",
        "\n",
        "        self.middle = nn.ModuleList([ResNetBlock(2 * ch, 2 * ch),\n",
        "                                     AttentionBlock(2 * ch),\n",
        "                                     ResNetBlock(2 * ch, 2 * ch)])\n",
        "\n",
        "        self.up = nn.ModuleList([ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 ResNetBlock(3 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(3 * ch, ch),\n",
        "                                 ResNetBlock(2 * ch, ch),\n",
        "                                 ResNetBlock(2 * ch, ch)])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(ch, in_ch, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        :param x (torch.Tensor): batch of images [B, C, H, W]\n",
        "        :param t (torch.Tensor): tensor of time steps (torch.long) [B]\n",
        "        \"\"\"\n",
        "\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = torch.nn.functional.silu(self.linear1(temb))\n",
        "        temb = self.linear2(temb)\n",
        "        assert temb.shape == (t.shape[0], self.ch*4)\n",
        "\n",
        "        x1 = self.conv1(x)\n",
        "\n",
        "        # Down\n",
        "        x2 = self.down[0](x1, temb)\n",
        "        x3 = self.down[1](x2, temb)\n",
        "        x4 = self.down[2](x3)\n",
        "        x5 = self.down[3](x4, temb)\n",
        "        x6 = self.down[4](x5)         # Attention\n",
        "        x7 = self.down[5](x6, temb)\n",
        "        x8 = self.down[6](x7)   # Attention\n",
        "        x9 = self.down[7](x8)\n",
        "        x10 = self.down[8](x9, temb)\n",
        "        x11 = self.down[9](x10, temb)\n",
        "        x12 = self.down[10](x11)\n",
        "        x13 = self.down[11](x12, temb)\n",
        "        x14 = self.down[12](x13, temb)\n",
        "\n",
        "        # Middle\n",
        "        x = self.middle[0](x14, temb)\n",
        "        x = self.middle[1](x)\n",
        "        x = self.middle[2](x, temb)\n",
        "\n",
        "        # Up\n",
        "        x = self.up[0](torch.cat((x, x14), dim=1), temb)\n",
        "        x = self.up[1](torch.cat((x, x13), dim=1), temb)\n",
        "        x = self.up[2](torch.cat((x, x12), dim=1), temb)\n",
        "        x = self.up[3](x)\n",
        "        x = self.up[4](torch.cat((x, x11), dim=1), temb)\n",
        "        x = self.up[5](torch.cat((x, x10), dim=1), temb)\n",
        "        x = self.up[6](torch.cat((x, x9), dim=1), temb)\n",
        "        x = self.up[7](x)\n",
        "        x = self.up[8](torch.cat((x, x8), dim=1), temb)\n",
        "        x = self.up[9](x)\n",
        "        x = self.up[10](torch.cat((x, x6), dim=1), temb)\n",
        "        x = self.up[11](x)\n",
        "        x = self.up[12](torch.cat((x, x4), dim=1), temb)\n",
        "        x = self.up[13](x)\n",
        "        x = self.up[14](x)\n",
        "        x = self.up[15](torch.cat((x, x3), dim=1), temb)\n",
        "        x = self.up[16](torch.cat((x, x2), dim=1), temb)\n",
        "        x = self.up[17](torch.cat((x, x1), dim=1), temb)\n",
        "\n",
        "        x = nn.functional.silu(nn.functional.group_norm(x, num_groups=32))\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "xfjDWKLvyDFh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel():\n",
        "\n",
        "    def __init__(self, T : int, model : nn.Module, device : str):\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        self.function_approximator = model.to(device)\n",
        "        self.device = device\n",
        "        self.beta = torch.linspace(1e-4, 0.02, T).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def training(self, batch_size, optimizer):\n",
        "        \"\"\"\n",
        "        Algorithm 1 in Denoising Diffusion Probabilistic Models\n",
        "        \"\"\"\n",
        "\n",
        "        x0 = sample_batch(batch_size, self.device)\n",
        "        t = torch.randint(1, self.T + 1, (batch_size,), device=self.device, dtype=torch.long)\n",
        "\n",
        "        eps = torch.randn_like(x0)\n",
        "\n",
        "        alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        eps_predicted = self.function_approximator(\n",
        "            torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * eps, t-1)\n",
        "\n",
        "        tensor1 =  x0 * torch.sqrt(alpha_bar_t)\n",
        "\n",
        "        tensor2 =  eps_predicted\n",
        "\n",
        "        loss = nn.functional.mse_loss(eps, eps_predicted)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item(), tensor1, eps_predicted\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), use_tqdm=True):\n",
        "\n",
        "        x = torch.randn((n_samples, image_channels, img_size[0], img_size[1]),\n",
        "                         device=self.device)\n",
        "\n",
        "        progress_bar = tqdm if use_tqdm else lambda x : x\n",
        "        for t in progress_bar(range(self.T, 0, -1)):\n",
        "            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
        "\n",
        "            t = torch.ones(n_samples, dtype=torch.long, device=self.device) * t\n",
        "\n",
        "            beta_t = self.beta[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_t = self.alpha[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "            mean = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(\n",
        "                1 - alpha_bar_t)) * self.function_approximator(x, t-1))\n",
        "            sigma = torch.sqrt(beta_t)\n",
        "            x = mean + sigma * z\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3y3iXpM28w9W"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "model = UNet()\n",
        "device = \"cuda\"\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "diffusion_model = DiffusionModel(1000, model, device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-y0YJJMM9G13"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=np.inf)\n",
        "training_loss = []\n",
        "for epoch in tqdm(range(35)):\n",
        "    loss, tensor1, pred = diffusion_model.training(batch_size, optimizer)\n",
        "    training_loss.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "    if epoch % 3 == 0:\n",
        "\n",
        "       tensor1 = tensor1[epoch,:,:,:].squeeze().cpu().numpy()\n",
        "       #print(tensor1)\n",
        "       plt.imshow(tensor1, cmap='gray', vmin=0., vmax=1.)\n",
        "       plt.savefig(f'/content/diffusion_models/figs/tensor1_{epoch}.png')\n",
        "       plt.close()\n",
        "\n",
        "       pred = pred[epoch,:,:,:].squeeze().cpu().detach().numpy()\n",
        "       plt.imshow(pred, cmap='gray', vmin=0., vmax=1.)\n",
        "       plt.savefig(f'/content/diffusion_models/figs/pred_{epoch}.png')\n",
        "       plt.close()\n",
        "\n",
        "       plt.plot(training_loss)\n",
        "       plt.savefig(f'/content/diffusion_models/figs/training_loss{epoch}.png')\n",
        "       plt.close()\n",
        "\n",
        "       plt.plot(training_loss[-1000:])\n",
        "       plt.savefig(f'/content/diffusion_models/figs/training_loss_cropped{epoch}.png')\n",
        "       plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #plt.plot(training_loss[-1000:])\n",
        "        #plt.savefig('training_loss_cropped.png')\n",
        "        #plt.close()\n",
        "\n",
        "\n",
        "    #if epoch % 5000 == 0:\n",
        "        #nb_images=81\n",
        "        #samples = diffusion_model.sampling(n_samples=nb_images, use_tqdm=False)\n",
        "        #plt.figure(figsize=(17, 17))\n",
        "        #for i in range(nb_images):\n",
        "            #plt.subplot(9, 9, 1 + i)\n",
        "            #plt.axis('off')\n",
        "            #plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')\n",
        "        #plt.savefig(f'samples_epoch_{epoch}.png')\n",
        "        #plt.close()\n",
        "\n",
        "       #torch.save(model.cpu(), f'model_paper2_epoch_{epoch}')\n",
        "        #model.cuda()"
      ],
      "metadata": {
        "id": "WmTPalyyBvT_",
        "outputId": "d5dc01f1-839d-43e0-fdf6-6aeec0fbabe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:23<00:00,  1.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Supongamos que 'training_loss' es tu lista que contiene tensores en el dispositivo CUDA\n",
        "# Movemos cada tensor de la lista a la CPU y los convertimos en arrays NumPy\n",
        "training_loss_cpu = [loss_tensor.cpu().numpy() for loss_tensor in training_loss]\n",
        "\n",
        "# Ahora puedes usar 'training_loss_cpu' para trazar tu gráfico con matplotlib\n",
        "for loss_np in training_loss_cpu:\n",
        "    plt.plot(loss_np)\n",
        "\n",
        "plt.savefig('training_loss.png')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "TQQ8qS9j7pkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ruta a la carpeta donde se encuentran las imágenes\n",
        "folder_path = '/content/diffusion_models'\n",
        "\n",
        "# Listar todos los archivos en la carpeta\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Iterar sobre todos los archivos y eliminar los que coinciden con el prefijo 'tensor1' o 'tensor2'\n",
        "for file in files:\n",
        "    if file.startswith('tensor1') or file.startswith('tensor2'):\n",
        "        os.remove(os.path.join(folder_path, file))\n",
        "\n"
      ],
      "metadata": {
        "id": "Eg_ew19sr-3f"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}